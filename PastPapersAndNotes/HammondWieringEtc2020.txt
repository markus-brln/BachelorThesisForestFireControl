##########################################
#FOREST FIRE CONTROL WITH LEARNING FROM  #
#DEMONSTRATION AND REINFORCEMENT LEARNING#
##########################################
by Travis Hammond, Dirk Jelle Schaap, Matthia Sabatelli, Marco A. Wiering


research question: 
	- how connectionist reinforcement learning (RL) can be used to allow an agent
	  to learn how to contain forest fires in a simulated environment by using a 
	  bulldozer to cut fire lines

- Q-Learning, SARSA, Dueling Q-Networks, Dueling-SARSA[new]
- RL: state, action, reward
- 10x10 or 14x14, 1 agent
- move==dig, because bulldozer (imo better approach, just adjust fire speed then)
- fire uses A* to spread to borders, 
  if no path found -> contained
- input for NN: agent pos, dug cells, burning cells
- filled "experience memory replay buffer" with
  demonstration data, for this clockwise moving algorithm
  used, only successful containments stored!!

- informal hyper-parameter search using Q-Learning

Results:
- demonstration data helped, but Q-Network benefited a lot,
  1st place in reward with 100 demo data, Dueling SARSA was
  1st with 1000 demo data
- on-policy (both SARSAs) better than off-policy (Qs)
- CNNs could be much more effective for handling very large environments
